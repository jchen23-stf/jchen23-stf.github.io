<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>How I Created Chinese Word Embeddings with Word2vec | 陈嘉辉｜Jiahui Chen</title><meta name="description" content="IntroWord2Vec is a common tool in Natural Language Processing (NLP) for generating word embeddings from text. Word embeddings are essentially vectors in a higher dimensional space - by translating wor"><meta name="author" content="Jiahui Chen"><meta name="copyright" content="Jiahui Chen"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2020/08/11/How-I-Created-Chinese-Word-Embeddings-with-Word2vec/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="How I Created Chinese Word Embeddings with Word2vec"><meta property="og:url" content="http://yoursite.com/2020/08/11/How-I-Created-Chinese-Word-Embeddings-with-Word2vec/"><meta property="og:site_name" content="陈嘉辉｜Jiahui Chen"><meta property="og:description" content="IntroWord2Vec is a common tool in Natural Language Processing (NLP) for generating word embeddings from text. Word embeddings are essentially vectors in a higher dimensional space - by translating wor"><meta property="og:image" content="https://pic.imgdb.cn/item/5f3271f214195aa594a74bb9.jpg"><meta property="article:published_time" content="2020-08-11T07:40:42.000Z"><meta property="article:modified_time" content="2020-08-24T06:47:10.964Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="A Tale of Three Cities | 三城记" href="http://yoursite.com/2020/08/15/A-Tale-of-Three-Cities-%E4%B8%89%E5%9F%8E%E8%AE%B0/"><link rel="next" title="My First Year at Stanford" href="http://yoursite.com/2020/07/06/My-First-Year-at-Stanford/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://pic.imgdb.cn/item/5ef9f9d514195aa5943e98dd.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">13</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intro"><span class="toc-number">1.</span> <span class="toc-text">Intro</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec"><span class="toc-number">2.</span> <span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intro-to-Word2Vec"><span class="toc-number">2.1.</span> <span class="toc-text">Intro to Word2Vec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-Does-the-Training-Work"><span class="toc-number">2.2.</span> <span class="toc-text">How Does the Training Work</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Getting-and-Processing-Data"><span class="toc-number">3.</span> <span class="toc-text">Getting and Processing Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-the-Gensim-Word2Vec-Model"><span class="toc-number">4.</span> <span class="toc-text">Using the Gensim Word2Vec Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Applying-the-model"><span class="toc-number">5.</span> <span class="toc-text">Applying the model</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://pic.imgdb.cn/item/5f3271f214195aa594a74bb9.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">陈嘉辉｜Jiahui Chen</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">How I Created Chinese Word Embeddings with Word2vec</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-08-11 00:40:42"><i class="far fa-calendar-alt fa-fw"></i> Created 2020-08-11</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-08-23 23:47:10"><i class="fas fa-history fa-fw"></i> Updated 2020-08-23</span></time></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Word2Vec is a common tool in Natural Language Processing (NLP) for generating word embeddings from text. Word embeddings are essentially vectors in a higher dimensional space - by translating words into these vectors, we can quantitatively represent the words within their context. The hope is to group similar words together just by looking at their context. In this blog post, I will walk through how I used the Word2Vec model to generate word embeddings using Gensim’s Word2Vec model. I will also show some of the applications of the model. </p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><h3 id="Intro-to-Word2Vec"><a href="#Intro-to-Word2Vec" class="headerlink" title="Intro to Word2Vec"></a>Intro to Word2Vec</h3><p>Word2Vec is a tool for Natural Language Processing - how to make machine “understand” human language. This task is incredibly difficult, as human languages are extremely complex, with unstructured expressions and weird combinations of words and slurs. One common example used to illustrate the weirdness of Chinese language is this joke:</p>
<blockquote>
<p>阿呆给领导送红包,领导说:你这是什么意思? 阿呆:意思意思. 领导:你这就不够意思了. 阿呆:小意思 ,小意思. 领导:你这人真有意思. 阿呆:没有别的意思. 领导:那我就不好意思了</p>
</blockquote>
<p>In this conversation, the Chinese word “意思” appeared numerous times, each with a different meaning. It is already difficult for someone without extensive experience living in China to pick up the meaning of this conversation, not to mention making machine understand it.<br>Then how can we let the machine learn the semantics of our weird and complex languages? The solution that Word2Vec comes up with is to use context. The idea behind it is best summarized by this maxim:</p>
<blockquote>
<p>You shall know a word by the company it keeps </p>
</blockquote>
<p>This may seem pretty intuitive but it is actually a very brilliant idea. Machine does no necessarily need to understand the semantics - all it needs to do is to infer whether it is common for a word to be at where it is given the context, which can be accomplished using a neural network.<br><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f327e5014195aa594ac4bac.jpg" alt=""><br>I will not go into details about how a neural network functions. If you are interested, I highly recommend <a href="">this video</a> by 3Blue1Brown - the visualization helped me understand the concept a lot. </p>
<p>In the neural network for Word2Vec, the inputs is an one-hot vector. The hidden layer does not have activation function, meaning it uses linear activation. The output layer is produced using Softmax - a classifier commonly used in machine learning task. It is important to note that what we want is not necessarily to use the model to accomplish any task; instead, we will take the parameters of the model in the form of a matrix - where each row would be a word vector that we want as result. </p>
<h3 id="How-Does-the-Training-Work"><a href="#How-Does-the-Training-Work" class="headerlink" title="How Does the Training Work"></a>How Does the Training Work</h3><p>That being said, there are commonly two ways to train the word2vec model, and word2vec uses both ways</p>
<ul>
<li>CBOW (Continuous Bag-of-Words) - where we use the context to infer the word</li>
<li>Skip Gram - where we use the word to infer the context<br>I will briefly talk about how the training functions with an example using the skip gram method.<br>Suppose the sentence that we want to train the model with is <blockquote>
<p>The quick brown fox jumps over the lazy dog.</p>
</blockquote>
</li>
</ul>
<p>At this point, we need to specify our skip window, which is the window of context that we want the machine to predict. Suppose we set that to 2, then we can generate the following training data set<br><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f327e9614195aa594ac6a6b.jpg" alt=""><br>We want to turn this into a classifier problem, so we turn the data set into one-hot encoding, generating a series of vectors where 1 appears on where the word is and 0 appears on everywhere else<br><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f327eb514195aa594ac75ef.jpg" alt=""><br>We put the training data into the neural network, and use back propagation to minimize the loss function,  hoping to make the resulting vector as close to our desired output. When the training is completed, we take out the embedding matrix, which will contain all the word vectors we want.<br><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f327ed114195aa594ac7fec.jpg" alt=""></p>
<h2 id="Getting-and-Processing-Data"><a href="#Getting-and-Processing-Data" class="headerlink" title="Getting and Processing Data"></a>Getting and Processing Data</h2><p>In the following sections, I will walk through how I trained a Word2Vec model using data from Peking University’s Chinese NLP data set.<br>I downloaded my dataset from <a href="">Kaggle</a> - a popular data science website containing a lot of useful dataset. My data consist of transcripts of Xinwenlianbo (新闻联播), an every-day TV news broadcast -  over the past few years.<br>Below is my code for processing the data and cutting them into individual words </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># this would be the output file </span></span><br><span class="line">f = io.open(<span class="string">'sentences.txt'</span>, mode = <span class="string">'w'</span>, encoding= <span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the csv file using pandas</span></span><br><span class="line">news = pd.read_csv(<span class="string">'chinese_news.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># take out all the news body </span></span><br><span class="line">x = list(news[<span class="string">'content'</span>])</span><br><span class="line"></span><br><span class="line">l = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># I want to skip all Chinese punctuactions</span></span><br><span class="line">skip_list = [<span class="string">''</span>,<span class="string">'。'</span>, <span class="string">'，'</span>, <span class="string">'、'</span>, <span class="string">'：'</span>, <span class="string">'“'</span>, <span class="string">'”'</span>, <span class="string">"《"</span>, <span class="string">"》"</span>, <span class="string">'\n'</span>,<span class="string">'；'</span>,<span class="string">'— —'</span>,<span class="string">'（'</span>, <span class="string">'）'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># looping through all the news stories, col would store individual news stories</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> x:</span><br><span class="line">    <span class="comment"># take the text out and remove white space</span></span><br><span class="line">    text = str(col).rstrip()</span><br><span class="line">    <span class="comment"># split the text by sentence</span></span><br><span class="line">    sentences = re.split(<span class="string">'。|！|\!|\.|？|\?'</span>, text)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(int(len(sentences) / <span class="number">2</span>)):</span><br><span class="line">        sentence = sentences[<span class="number">2</span> * i] + sentences[<span class="number">2</span> * i + <span class="number">1</span>]</span><br><span class="line">        <span class="comment"># skip all the punctuation</span></span><br><span class="line">        <span class="keyword">for</span> skip_char <span class="keyword">in</span> skip_list:</span><br><span class="line">            sentence = sentence.replace(skip_char, <span class="string">''</span>)</span><br><span class="line">        <span class="comment"># I use jieba, a Chinese language segmentation tool, to cut the sentence into words</span></span><br><span class="line">        seg_list = list(jieba.cut(sentence))</span><br><span class="line">        <span class="keyword">for</span> temp_term <span class="keyword">in</span> seg_list:</span><br><span class="line">            l.append(temp_term)</span><br><span class="line">        <span class="comment"># output format would be words seperated by white spaces</span></span><br><span class="line">        res = <span class="string">" "</span>.join(l) + <span class="string">'\n'</span></span><br><span class="line">        res.lstrip()</span><br><span class="line">        <span class="comment"># write the result into output file</span></span><br><span class="line">        f.write(res)</span><br><span class="line">        l = []</span><br><span class="line"></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p>One thing to note is that I used jieba for word segmentation. You can find extensive instructions on how to pip install it online. Another common tool is hanlp (which is probably more advanced too). For simplicity I sticked to jieba.<br>After the program is run the output file should look like this<br><img src= "/img/loading.gif" data-src="file:///.file/id=6571367.21844725" alt=""></p>
<h2 id="Using-the-Gensim-Word2Vec-Model"><a href="#Using-the-Gensim-Word2Vec-Model" class="headerlink" title="Using the Gensim Word2Vec Model"></a>Using the Gensim Word2Vec Model</h2><p>The use of Genism Word2Vec Model is fairly simple and straight-forward after pre-processing. You can also find instructions on how to pip install gensim online - it is quite easy from my experience. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line">a = io.open(<span class="string">"sentences.txt"</span>, mode=<span class="string">"r+"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">model = Word2Vec(LineSentence(a), size=<span class="number">400</span>, window = <span class="number">5</span>, min_count= <span class="number">5</span>, workers= multiprocessing.cpu_count() - <span class="number">4</span>)</span><br><span class="line">model.save(<span class="string">'word2.model'</span>)</span><br><span class="line">model.wv.save_word2vec_format(<span class="string">'word2vec.vector'</span>, binary=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>As you can see, there are several parameters you can choose when training the model</p>
<ul>
<li>size: the dimension of output vector. In most cases higher dimensions correlate with increased accuracy - the intuition behind that more dimension means the vector can store more nuanced relations. I set it to 400, which is the recommended default.</li>
<li>window: the size of window that you want to use during training. That is, how many words do you want the machine to be looking at beyond the current word. A larger window means the algorithm is considering more context and vice versa.</li>
<li>min count: this is used in the initial processing. Words with frequency less than the min count would be discarded before the training begins.</li>
<li>workers: how many cpu workers you want to use in the training. </li>
</ul>
<p>After the training, I stored the model in two formats</p>
<ul>
<li>.model: models stored in this format can be retrained, but the format can not be decoded for you to see</li>
<li>.vector: models stored in this format cannot be retrained, but the format can be decoded for you to see<br>My .vector file looks like this - you can see each word is associated with a word vector<br><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f32b55614195aa594bf8845.png" alt=""><h2 id="Applying-the-model"><a href="#Applying-the-model" class="headerlink" title="Applying the model"></a>Applying the model</h2>The last step would be to use the model to do something. As it turns out Gensim lets you do a lot of things with your model. Below are some examples<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(<span class="string">'word2vec.vector'</span>, binary=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#print the word vector associated with the word</span></span><br><span class="line">print(model[<span class="string">"中国"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># print top 10 most similar words to the input word</span></span><br><span class="line">print(model.most_similar(positive=<span class="string">'广东'</span>, negative=<span class="literal">None</span>, topn=<span class="number">10</span>, restrict_vocab=<span class="literal">None</span>, indexer=<span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># compare similarities between sentences </span></span><br><span class="line">sent1 = [<span class="string">'中国'</span>,<span class="string">'科技'</span>,<span class="string">'发展'</span>,<span class="string">'要'</span>,<span class="string">'靠'</span>,<span class="string">'人才'</span>]</span><br><span class="line">sent2 = [<span class="string">'美国'</span>,<span class="string">'经济'</span>,<span class="string">'增长'</span>,<span class="string">'速度'</span>,<span class="string">'提升'</span>,<span class="string">'快'</span>]</span><br><span class="line">sent3 = [<span class="string">'我'</span>,<span class="string">'今天'</span>,<span class="string">'快乐'</span>,<span class="string">'生活'</span>,<span class="string">'快乐'</span>,<span class="string">'学习'</span>]</span><br><span class="line">sim1 = model.n_similarity(sent1, sent2)</span><br><span class="line">sim2 = model.n_similarity(sent1, sent3)</span><br><span class="line">sim3 = model.n_similarity(sent2, sent3)</span><br><span class="line">print(<span class="string">"similarity between"</span>, sent1, sent2, sim1)</span><br><span class="line">print(<span class="string">"similarity between"</span>, sent1, sent3, sim2)</span><br><span class="line">print(<span class="string">"similarity between"</span>, sent2, sent3, sim3)</span><br></pre></td></tr></table></figure>
The output looks like this <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">-0.05930993</span> <span class="number">-0.60154545</span> <span class="number">-0.19779043</span> <span class="number">-0.48194197</span> <span class="number">-0.30343688</span> <span class="number">-1.0903504</span></span><br><span class="line">  <span class="number">1.7381277</span>  <span class="number">-1.1709697</span>   <span class="number">0.2171865</span>  <span class="number">-0.59634805</span>  <span class="number">0.6168709</span>   <span class="number">0.36309278</span></span><br><span class="line"> <span class="number">-0.00500764</span>  <span class="number">0.51317024</span> <span class="number">-0.17959161</span> <span class="number">-0.8421643</span>  <span class="number">-0.7322384</span>   <span class="number">0.689643</span></span><br><span class="line">  <span class="number">0.31742564</span>  <span class="number">0.65299606</span> <span class="number">-0.43274835</span>  <span class="number">0.26696515</span>  <span class="number">0.41932794</span>  <span class="number">0.32154498</span></span><br><span class="line">  <span class="number">1.3054777</span>   <span class="number">1.3687644</span>   <span class="number">0.6937966</span>  <span class="number">-0.58396184</span> <span class="number">-0.35994187</span>  <span class="number">1.5102268</span></span><br><span class="line"> <span class="number">-0.3021141</span>   <span class="number">0.35180223</span>  <span class="number">0.98756677</span> <span class="number">-0.6793006</span>  <span class="number">-0.8039679</span>  <span class="number">-0.11551007</span></span><br><span class="line">  <span class="number">0.23520286</span> <span class="number">-0.6949743</span>  <span class="number">-0.4775846</span>   <span class="number">0.62763804</span>  <span class="number">0.32157004</span>  <span class="number">0.10934389</span></span><br><span class="line"> <span class="number">-0.83816004</span> <span class="number">-1.005785</span>   <span class="number">-0.26850745</span>  <span class="number">0.7644115</span>   <span class="number">0.8944372</span>  <span class="number">-1.192454</span></span><br><span class="line">  <span class="number">0.1794393</span>  <span class="number">-0.5560042</span>   <span class="number">0.11054779</span> <span class="number">-0.2966826</span>   <span class="number">0.7419602</span>   <span class="number">0.7677035</span></span><br><span class="line">  <span class="number">0.13389288</span>  <span class="number">0.15937632</span> <span class="number">-0.11777601</span>  <span class="number">0.12296207</span> <span class="number">-0.64838636</span> <span class="number">-0.5888581</span></span><br><span class="line"> <span class="number">-0.2861753</span>   <span class="number">1.6017598</span>   <span class="number">0.6768199</span>  <span class="number">-1.1618366</span>   <span class="number">0.9654343</span>   <span class="number">1.6714641</span></span><br><span class="line">  <span class="number">0.5946807</span>   <span class="number">0.41378683</span> <span class="number">-1.1309861</span>  <span class="number">-0.5350951</span>  <span class="number">-0.61663336</span> <span class="number">-0.74870783</span></span><br><span class="line"> <span class="number">-0.6871212</span>   <span class="number">0.17910142</span>  <span class="number">0.7646165</span>   <span class="number">0.3806211</span>  <span class="number">-0.5449704</span>   <span class="number">0.5622143</span></span><br><span class="line"> <span class="number">-0.97382987</span>  <span class="number">0.5746229</span>   <span class="number">0.985208</span>    <span class="number">1.2262727</span>  <span class="number">-0.9806304</span>   <span class="number">1.2106696</span></span><br><span class="line"> <span class="number">-0.3302971</span>   <span class="number">0.27300367</span>  <span class="number">0.06429467</span>  <span class="number">0.48375</span>     <span class="number">0.521364</span>    <span class="number">0.9094922</span></span><br><span class="line">  <span class="number">0.11546493</span>  <span class="number">0.09551446</span>  <span class="number">0.16652733</span>  <span class="number">0.03306641</span> <span class="number">-0.07538961</span> <span class="number">-1.0116953</span></span><br><span class="line"> <span class="number">-0.13459866</span>  <span class="number">1.8211585</span>  <span class="number">-0.6950327</span>   <span class="number">1.2637784</span>  <span class="number">-0.52912796</span> <span class="number">-0.04460784</span></span><br><span class="line">  <span class="number">1.0214093</span>   <span class="number">0.10162195</span> <span class="number">-0.91459376</span> <span class="number">-0.17390226</span> <span class="number">-0.17388315</span> <span class="number">-0.2754795</span></span><br><span class="line">  <span class="number">0.20606285</span>  <span class="number">0.15042375</span>  <span class="number">0.13845378</span>  <span class="number">0.02339607</span> <span class="number">-1.5108862</span>  <span class="number">-0.45250568</span></span><br><span class="line"> <span class="number">-0.9359936</span>   <span class="number">0.49126223</span>  <span class="number">1.1190046</span>  <span class="number">-0.87515837</span> <span class="number">-0.57949454</span>  <span class="number">0.6871173</span></span><br><span class="line"> <span class="number">-0.12499838</span>  <span class="number">1.9581158</span>   <span class="number">0.95917594</span> <span class="number">-0.10627772</span>  <span class="number">0.54702</span>    <span class="number">-0.9470129</span></span><br><span class="line"> <span class="number">-1.6031312</span>  <span class="number">-0.07724522</span> <span class="number">-0.05718271</span>  <span class="number">0.5277461</span>  <span class="number">-0.06041054</span> <span class="number">-0.23856659</span></span><br><span class="line">  <span class="number">0.5542616</span>  <span class="number">-1.5007663</span>  <span class="number">-0.2780823</span>  <span class="number">-0.2115478</span>  <span class="number">-1.4245342</span>  <span class="number">-0.01399347</span></span><br><span class="line">  <span class="number">0.60007745</span> <span class="number">-0.01684805</span> <span class="number">-1.6818323</span>  <span class="number">-0.1931889</span>  <span class="number">-1.5237374</span>  <span class="number">-1.4075873</span></span><br><span class="line">  <span class="number">0.02254261</span> <span class="number">-0.5751521</span>  <span class="number">-0.62415326</span>  <span class="number">0.3235581</span>   <span class="number">0.06705747</span> <span class="number">-1.0198073</span></span><br><span class="line"> <span class="number">-0.6251598</span>  <span class="number">-0.91364276</span> <span class="number">-0.176965</span>   <span class="number">-0.06620737</span>  <span class="number">1.8866454</span>  <span class="number">-0.24774651</span></span><br><span class="line"> <span class="number">-0.21445335</span>  <span class="number">1.1362627</span>   <span class="number">1.273662</span>    <span class="number">1.3616595</span>  <span class="number">-1.2018119</span>   <span class="number">1.4867073</span></span><br><span class="line"> <span class="number">-0.07305669</span>  <span class="number">0.16995254</span>  <span class="number">0.70268124</span>  <span class="number">0.04884331</span>  <span class="number">0.04264761</span> <span class="number">-0.24667963</span></span><br><span class="line"> <span class="number">-0.03846245</span> <span class="number">-0.09678071</span> <span class="number">-0.27120808</span> <span class="number">-0.05220251</span>  <span class="number">0.08652628</span>  <span class="number">0.49769568</span></span><br><span class="line"> <span class="number">-0.5352079</span>   <span class="number">0.46929055</span>  <span class="number">0.6785873</span>   <span class="number">0.49454334</span> <span class="number">-1.4178201</span>   <span class="number">0.4647789</span></span><br><span class="line">  <span class="number">1.2762723</span>  <span class="number">-1.1194816</span>  <span class="number">-1.010007</span>   <span class="number">-0.6022205</span>   <span class="number">0.6323132</span>  <span class="number">-0.16102295</span></span><br><span class="line">  <span class="number">1.1645333</span>   <span class="number">0.9596903</span>  <span class="number">-0.51386905</span>  <span class="number">0.588219</span>    <span class="number">0.08259875</span>  <span class="number">0.3908056</span></span><br><span class="line">  <span class="number">1.0106333</span>  <span class="number">-0.14321563</span>  <span class="number">0.9018996</span>  <span class="number">-1.0544982</span>  <span class="number">-0.23611164</span>  <span class="number">1.0572827</span></span><br><span class="line">  <span class="number">0.18362053</span>  <span class="number">0.39346454</span>  <span class="number">1.0405265</span>  <span class="number">-0.5581456</span>   <span class="number">0.17081226</span> <span class="number">-0.24262072</span></span><br><span class="line"> <span class="number">-0.15824328</span> <span class="number">-0.5129465</span>  <span class="number">-0.20730579</span>  <span class="number">0.49548322</span> <span class="number">-0.7830268</span>   <span class="number">1.1653224</span></span><br><span class="line">  <span class="number">0.28673372</span>  <span class="number">1.5712413</span>  <span class="number">-0.9241767</span>  <span class="number">-1.150959</span>   <span class="number">-0.58597785</span> <span class="number">-0.6976993</span></span><br><span class="line">  <span class="number">0.8195765</span>   <span class="number">0.88995826</span> <span class="number">-0.53419846</span> <span class="number">-0.30637687</span> <span class="number">-0.6004372</span>  <span class="number">-0.236429</span></span><br><span class="line"> <span class="number">-0.7772929</span>  <span class="number">-0.03667458</span>  <span class="number">0.45034775</span> <span class="number">-0.67187023</span> <span class="number">-0.4030743</span>  <span class="number">-0.8062104</span></span><br><span class="line">  <span class="number">2.623117</span>    <span class="number">0.02814003</span>  <span class="number">0.00678085</span>  <span class="number">0.47618607</span> <span class="number">-1.2426374</span>   <span class="number">1.2759159</span></span><br><span class="line">  <span class="number">0.56348497</span> <span class="number">-0.0306225</span>   <span class="number">0.07943363</span>  <span class="number">0.33734903</span>  <span class="number">0.74658936</span> <span class="number">-0.74653673</span></span><br><span class="line"> <span class="number">-0.6815518</span>  <span class="number">-0.06754778</span> <span class="number">-1.8862988</span>  <span class="number">-1.723702</span>    <span class="number">0.39478</span>    <span class="number">-0.3838985</span></span><br><span class="line"> <span class="number">-0.24430585</span> <span class="number">-0.87098897</span>  <span class="number">0.12656392</span>  <span class="number">0.3491659</span>  <span class="number">-0.06678118</span>  <span class="number">0.8528555</span></span><br><span class="line">  <span class="number">0.62753266</span>  <span class="number">0.80654687</span> <span class="number">-0.10528732</span> <span class="number">-0.06510979</span>  <span class="number">0.60498166</span>  <span class="number">1.1265533</span></span><br><span class="line">  <span class="number">0.12893651</span>  <span class="number">0.22323228</span> <span class="number">-0.43933755</span>  <span class="number">0.30504924</span>  <span class="number">0.27831098</span> <span class="number">-0.9089467</span></span><br><span class="line"> <span class="number">-0.25216293</span>  <span class="number">0.45882642</span>  <span class="number">1.0253651</span>   <span class="number">0.83845913</span>  <span class="number">0.19217832</span> <span class="number">-0.25366136</span></span><br><span class="line"> <span class="number">-1.1501366</span>  <span class="number">-0.59544957</span>  <span class="number">0.71397233</span> <span class="number">-0.33581296</span> <span class="number">-0.3301465</span>   <span class="number">1.0448421</span></span><br><span class="line"> <span class="number">-0.98036987</span> <span class="number">-0.5961618</span>   <span class="number">1.4408491</span>  <span class="number">-0.89590365</span> <span class="number">-1.4903147</span>   <span class="number">0.48930526</span></span><br><span class="line"> <span class="number">-0.20611231</span>  <span class="number">0.80190253</span> <span class="number">-1.17446</span>    <span class="number">-0.44388193</span> <span class="number">-0.864709</span>    <span class="number">0.7994709</span></span><br><span class="line">  <span class="number">0.6211015</span>   <span class="number">1.0785302</span>  <span class="number">-0.39803782</span> <span class="number">-0.06914762</span> <span class="number">-1.2612135</span>  <span class="number">-0.43342596</span></span><br><span class="line">  <span class="number">0.9597424</span>  <span class="number">-0.4313669</span>  <span class="number">-0.83414686</span>  <span class="number">0.40143782</span>  <span class="number">0.085173</span>   <span class="number">-0.1414892</span></span><br><span class="line"> <span class="number">-0.244692</span>    <span class="number">1.5626423</span>  <span class="number">-0.04647842</span>  <span class="number">1.294185</span>    <span class="number">0.70646983</span> <span class="number">-0.93281174</span></span><br><span class="line">  <span class="number">1.7463619</span>  <span class="number">-1.5049967</span>   <span class="number">1.2497619</span>  <span class="number">-0.4949764</span>  <span class="number">-1.1369902</span>   <span class="number">0.583246</span></span><br><span class="line"> <span class="number">-1.9625047</span>  <span class="number">-1.9645227</span>   <span class="number">0.39969036</span>  <span class="number">0.1845216</span>  <span class="number">-2.224353</span>    <span class="number">0.36199805</span></span><br><span class="line"> <span class="number">-1.4422947</span>  <span class="number">-1.8587517</span>   <span class="number">1.0950716</span>   <span class="number">1.6715609</span>  <span class="number">-1.8970809</span>   <span class="number">1.2119989</span></span><br><span class="line"> <span class="number">-0.9918985</span>   <span class="number">0.2168934</span>  <span class="number">-0.03940273</span> <span class="number">-1.4473706</span>  <span class="number">-0.1715694</span>   <span class="number">0.40139624</span></span><br><span class="line"> <span class="number">-0.20749897</span> <span class="number">-0.23848538</span>  <span class="number">0.49070346</span>  <span class="number">1.8710481</span>  <span class="number">-0.22102888</span>  <span class="number">1.8875043</span></span><br><span class="line">  <span class="number">0.1575313</span>  <span class="number">-2.143685</span>    <span class="number">0.9722614</span>  <span class="number">-0.18779267</span> <span class="number">-1.5262072</span>   <span class="number">0.01395513</span></span><br><span class="line">  <span class="number">0.6219663</span>   <span class="number">0.6200599</span>   <span class="number">1.0746591</span>  <span class="number">-0.6949839</span>  <span class="number">-0.11403554</span> <span class="number">-0.11398692</span></span><br><span class="line"> <span class="number">-1.1922437</span>  <span class="number">-0.6713476</span>  <span class="number">-0.04333466</span>  <span class="number">1.6420592</span>   <span class="number">0.21196647</span> <span class="number">-0.6093193</span></span><br><span class="line"> <span class="number">-0.13868634</span>  <span class="number">0.8807369</span>   <span class="number">0.5370432</span>  <span class="number">-1.0475727</span>  <span class="number">-0.38448784</span> <span class="number">-0.03109699</span></span><br><span class="line"> <span class="number">-0.665416</span>    <span class="number">0.11490192</span> <span class="number">-0.46147627</span>  <span class="number">1.2118089</span>   <span class="number">0.78576756</span>  <span class="number">0.7781474</span></span><br><span class="line">  <span class="number">0.13457988</span>  <span class="number">0.0555984</span>   <span class="number">0.01180293</span>  <span class="number">0.69014937</span>  <span class="number">0.44752008</span>  <span class="number">1.715098</span></span><br><span class="line">  <span class="number">0.10089613</span> <span class="number">-0.8013588</span>   <span class="number">0.53294414</span>  <span class="number">0.12885724</span>  <span class="number">0.3112086</span>   <span class="number">1.6809343</span></span><br><span class="line">  <span class="number">0.00383066</span> <span class="number">-0.9831795</span>   <span class="number">1.19722</span>    <span class="number">-1.5555032</span>   <span class="number">0.3774249</span>  <span class="number">-0.8096407</span></span><br><span class="line">  <span class="number">0.5436396</span>  <span class="number">-0.33276993</span> <span class="number">-1.1977743</span>  <span class="number">-0.2084381</span>  <span class="number">-0.9183242</span>  <span class="number">-0.37455353</span></span><br><span class="line"> <span class="number">-1.7807964</span>   <span class="number">0.5187864</span>   <span class="number">0.22164525</span>  <span class="number">0.48849887</span>  <span class="number">0.7664638</span>  <span class="number">-0.46407062</span></span><br><span class="line"> <span class="number">-0.7822278</span>  <span class="number">-0.12114441</span>  <span class="number">0.8138251</span>  <span class="number">-0.6337241</span> ]</span><br><span class="line">[(<span class="string">'福建'</span>, <span class="number">0.9159765243530273</span>), (<span class="string">'湖北'</span>, <span class="number">0.9119961261749268</span>), (<span class="string">'江苏'</span>, <span class="number">0.9061319828033447</span>), (<span class="string">'安徽'</span>, <span class="number">0.8997495174407959</span>), (<span class="string">'陕西'</span>, <span class="number">0.8922166228294373</span>), (<span class="string">'广西'</span>, <span class="number">0.8836355209350586</span>), (<span class="string">'河南'</span>, <span class="number">0.8768578171730042</span>), (<span class="string">'浙江'</span>, <span class="number">0.8689074516296387</span>), (<span class="string">'江西'</span>, <span class="number">0.868665337562561</span>), (<span class="string">'山东'</span>, <span class="number">0.8685473203659058</span>)]</span><br><span class="line">similarity between [<span class="string">'中国'</span>, <span class="string">'科技'</span>, <span class="string">'发展'</span>, <span class="string">'要'</span>, <span class="string">'靠'</span>, <span class="string">'人才'</span>] [<span class="string">'美国'</span>, <span class="string">'经济'</span>, <span class="string">'增长'</span>, <span class="string">'速度'</span>, <span class="string">'提升'</span>, <span class="string">'快'</span>] <span class="number">0.4123175</span></span><br><span class="line">similarity between [<span class="string">'中国'</span>, <span class="string">'科技'</span>, <span class="string">'发展'</span>, <span class="string">'要'</span>, <span class="string">'靠'</span>, <span class="string">'人才'</span>] [<span class="string">'我'</span>, <span class="string">'今天'</span>, <span class="string">'快乐'</span>, <span class="string">'生活'</span>, <span class="string">'快乐'</span>, <span class="string">'学习'</span>] <span class="number">0.30470887</span></span><br><span class="line">similarity between [<span class="string">'美国'</span>, <span class="string">'经济'</span>, <span class="string">'增长'</span>, <span class="string">'速度'</span>, <span class="string">'提升'</span>, <span class="string">'快'</span>] [<span class="string">'我'</span>, <span class="string">'今天'</span>, <span class="string">'快乐'</span>, <span class="string">'生活'</span>, <span class="string">'快乐'</span>, <span class="string">'学习'</span>] <span class="number">0.05903372</span></span><br><span class="line"></span><br><span class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></span><br></pre></td></tr></table></figure>
The results are pretty good - when I try to output the most similar words to “广东” (Guangdong), all the words are similar Chinese provinces. Also Word2Vec has broader applications than this. Once you have all the word embeddings, you can easily compute similarities between sentences, paragraphs, and even texts. Indeed, Word2Vec was used in a variety of NLP tasks - classification, emotion detection, name entity recognition (NER). The reason I said it was is because it has been replaced by more advanced models like BERT. However, Word2Vector has been a foundation of many NLP models including BERT, so understanding the mechanisms of this model would definitely help you take your first step in NLP. </li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jiahui Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/08/11/How-I-Created-Chinese-Word-Embeddings-with-Word2vec/">http://yoursite.com/2020/08/11/How-I-Created-Chinese-Word-Embeddings-with-Word2vec/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://pic.imgdb.cn/item/616138712ab3f51d915cfe98.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/15/A-Tale-of-Three-Cities-%E4%B8%89%E5%9F%8E%E8%AE%B0/"><img class="prev-cover" data-src="https://pic.imgdb.cn/item/5f38ac4514195aa594385bf6.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">A Tale of Three Cities | 三城记</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/06/My-First-Year-at-Stanford/"><img class="next-cover" data-src="https://pic.imgdb.cn/item/5f02dab514195aa594ea8c3c.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">My First Year at Stanford</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Jiahui Chen</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script></body></html>