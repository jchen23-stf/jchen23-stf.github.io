<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Illustrated BERT Fine-Tuning Process | 陈嘉辉｜Jiahui Chen</title><meta name="description" content="Intro - BERT and FinetuningIf there is ever an NLP buzzword for the past few years, it should be BERT. You can see it in most blogs, products, and even research in the field of Natural Language Proces"><meta name="author" content="Jiahui Chen"><meta name="copyright" content="Jiahui Chen"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2020/08/25/Fine-tuning-BERT-Classifier/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="Illustrated BERT Fine-Tuning Process"><meta property="og:url" content="http://yoursite.com/2020/08/25/Fine-tuning-BERT-Classifier/"><meta property="og:site_name" content="陈嘉辉｜Jiahui Chen"><meta property="og:description" content="Intro - BERT and FinetuningIf there is ever an NLP buzzword for the past few years, it should be BERT. You can see it in most blogs, products, and even research in the field of Natural Language Proces"><meta property="og:image" content="https://pic.imgdb.cn/item/5f50f000160a154a674024cd.jpg"><meta property="article:published_time" content="2020-08-25T08:16:16.000Z"><meta property="article:modified_time" content="2020-09-03T13:41:26.749Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="next" title="A Tale of Three Cities | 三城记" href="http://yoursite.com/2020/08/16/A-Tale-of-Three-Cities-%E4%B8%89%E5%9F%8E%E8%AE%B0/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://pic.imgdb.cn/item/5ef9f9d514195aa5943e98dd.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intro-BERT-and-Finetuning"><span class="toc-number">1.</span> <span class="toc-text">Intro - BERT and Finetuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Run-classifier-source-code-walkthrough"><span class="toc-number">2.</span> <span class="toc-text">Run classifier source code walkthrough</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reading-and-pre-processing-data"><span class="toc-number">2.1.</span> <span class="toc-text">Reading and pre-processing data</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Demo-similarity-between-two-sentences"><span class="toc-number">3.</span> <span class="toc-text">Demo: similarity between two sentences</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://pic.imgdb.cn/item/5f50f000160a154a674024cd.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">陈嘉辉｜Jiahui Chen</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Illustrated BERT Fine-Tuning Process</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-08-25 16:16:16"><i class="far fa-calendar-alt fa-fw"></i> Created 2020-08-25</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-09-03 21:41:26"><i class="fas fa-history fa-fw"></i> Updated 2020-09-03</span></time></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="Intro-BERT-and-Finetuning"><a href="#Intro-BERT-and-Finetuning" class="headerlink" title="Intro - BERT and Finetuning"></a>Intro - BERT and Finetuning</h2><p>If there is ever an NLP buzzword for the past few years, it should be BERT. You can see it in most blogs, products, and even research in the field of Natural Language Processing(NLP).  BERT (Bi-Directional Encoder Representations from Transformers) is a pre-trained NLP model developed by Google back in 2016. The idea behinds BERT follows the general philosophy of NLP work in recent decades - generating embeddings that contain rich relations between words in human languages and using the embeddings for a variety of downstream tasks. Please see my previous blog post <a href=""></a> for a more detailed walk-through of what word embeddings are and how they can be used.<br><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f50f000160a154a674024cd.jpg" alt="Concidentally BERT is also the name of a character from Sesame Street,and so are names of many other NLP models"><br>BERT addresses a series of problems that previous NLP models have. Models like Word2Vec can generate pretty accurate word embeddings with enough training data - for example, producing vectors such that V(king) - V(man) + V(woman) = V(queen) - but it does not address the problem of the same word having multiple meanings. Because it only looks at a fixed window when doing training, the only thing that determines the embedding of a given word is the words that appear around them in the training data set. Therefore, sentences like “I withdrew some money from the bank” and “I was sleeping on the bank of the river“ would produce very similar embedding for the word “bank” despite it having essentially different meanings in the two sentences. BERT addresses this problem by using what’s called the Transformer architecture with self-attention layers (it is an encoder of the Transformer architecture to be precise). The architecture is a bit complex to understand for beginners in NLP, and I would recommend reading this blog post for a detailed introduction of the architecture <a href="illustrated%20bert"></a>.  Essentially, what BERT does is that it lets every word to generate “attentions” with other words, including itself, in the sentence. This would generate dynamic relations between the words that will indicate their semantics in a given sentence.  </p>
<p>The original BERT pre-trained models have two versions - BERT BASE and BERT LARGE. As their names indicate, the former in the base model comparable in size to the OpenAI Transformer, and the latter is a ridiculously huge model that achieved the state of the art results reported in the original paper. Both models are incredibly accurate by themselves, but BERT has yet another design that makes it even more powerful for downstream tasks - fine-tuning. </p>
<p>Fine-tuning is the process where developers train extra layers outside of the BERT pre-trained models to achieve specific tasks. These tasks include text classification, sentiment analysis, name-entity recognition, question answering, and more. Google actually provides code for several fine-tuning processes in its original open-sourced BERT code, and today I am going to look into the source code of arguably the most common fine-tuning task used for BERT - classification.<br><img src= "/img/loading.gif" data-src="http://jalammar.github.io/images/BERT-classification-spam.png" alt="From http://jalammar.github.io/illustrated-bert/"><br>As the above image illustrates, for classifiers, we are adding another layer for classification (e.g. softmax) based on the pre-trained BERT model. In training the whole model altogether, there will be minimal changes to the original BERT models while achieving state-of-art text classification performance. </p>
<h2 id="Run-classifier-source-code-walkthrough"><a href="#Run-classifier-source-code-walkthrough" class="headerlink" title="Run classifier source code walkthrough"></a>Run classifier source code walkthrough</h2><p>The source code I will be referencing below comes from the run_classifier.py and modeling.py files under the original BERT source code folder open-sourced by Google. I ran the programs in debug mode to trace through every step of the process. </p>
<h3 id="Reading-and-pre-processing-data"><a href="#Reading-and-pre-processing-data" class="headerlink" title="Reading and pre-processing data"></a>Reading and pre-processing data</h3><p>Below is the main function of the run_classifier.py program. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  flags.mark_flag_as_required(<span class="string">"data_dir"</span>)</span><br><span class="line">  flags.mark_flag_as_required(<span class="string">"task_name"</span>)</span><br><span class="line">  flags.mark_flag_as_required(<span class="string">"bert_hub_module_handle"</span>)</span><br><span class="line">  flags.mark_flag_as_required(<span class="string">"output_dir"</span>)</span><br><span class="line">  tf.app.run()</span><br></pre></td></tr></table></figure>
<p>Let’s start by setting a break point at tf.app.run() and run the program. After some initiations for tensowflow, you should go into the below blog of code. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> FLAGS.do_train:</span><br><span class="line">  train_examples = processor.get_train_examples(FLAGS.data_dir) <span class="comment"># read the data</span></span><br><span class="line">  num_train_steps = int(</span><br><span class="line">      len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)</span><br><span class="line">  num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion) <span class="comment"># make the learning rate small at first</span></span><br></pre></td></tr></table></figure>
<p>This code block is where BERT reads in the data and determines the train steps. The num_warmup_steps variable seems a bit strange here. Like its name suggests, what it does is that it makes the learning rate small at first, and then use the normal learning rate after warmup.</p>
<p>After setting up the initial parameters for later training, the next step is preparing the data into the format that can be fed into BERT. This is achieved with a call to the following function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_based_convert_examples_to_features(</span><br><span class="line">        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)</span><br></pre></td></tr></table></figure>
<p>Now we step into the function to see exactly how that works</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_based_convert_examples_to_features</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    examples, label_list, max_seq_length, tokenizer, output_file)</span>:</span></span><br><span class="line">  <span class="string">"""Convert a set of `InputExample`s to a TFRecord file."""</span></span><br><span class="line"></span><br><span class="line">  writer = tf.python_io.TFRecordWriter(output_file) <span class="comment">#convert it to TFRecorder to make it run faster</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (ex_index, example) <span class="keyword">in</span> enumerate(examples): <span class="comment">#iterate to read through the data </span></span><br><span class="line">    <span class="keyword">if</span> ex_index % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">      tf.logging.info(<span class="string">"Writing example %d of %d"</span> % (ex_index, len(examples)))</span><br><span class="line"></span><br><span class="line">    feature = convert_single_example(ex_index, example, label_list,</span><br><span class="line">                                     max_seq_length, tokenizer)</span><br></pre></td></tr></table></figure>
<p>This function accomplishes several things: it converts the output file to TFRencoder to make the program run faster, updates the user by printing the process after every 10000 iterations, and lastly, calls again to the convert_single_example function (#decomposition). </p>
<p>Now we will step in to the convert_single_example function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_single_example</span><span class="params">(ex_index, example, label_list, max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                           tokenizer)</span>:</span></span><br><span class="line">  <span class="string">"""Converts a single `InputExample` into a single `InputFeatures`."""</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> isinstance(example, PaddingInputExample):</span><br><span class="line">    <span class="keyword">return</span> InputFeatures(</span><br><span class="line">        input_ids=[<span class="number">0</span>] * max_seq_length,</span><br><span class="line">        input_mask=[<span class="number">0</span>] * max_seq_length,</span><br><span class="line">        segment_ids=[<span class="number">0</span>] * max_seq_length,</span><br><span class="line">        label_id=<span class="number">0</span>,</span><br><span class="line">        is_real_example=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  label_map = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> (i, label) <span class="keyword">in</span> enumerate(label_list): <span class="comment">#construct the label</span></span><br><span class="line">    label_map[label] = i</span><br><span class="line"></span><br><span class="line">  tokens_a = tokenizer.tokenize(example.text_a) <span class="comment">#tokenize the first sentence with wordpiece</span></span><br><span class="line">  tokens_b = <span class="literal">None</span></span><br><span class="line">  <span class="keyword">if</span> example.text_b:</span><br><span class="line">    tokens_b = tokenizer.tokenize(example.text_b) <span class="comment">#tokenize the second sentence with wordpiece (if the second sentence exists)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> tokens_b:</span><br><span class="line">    <span class="comment"># Modifies `tokens_a` and `tokens_b` in place so that the total</span></span><br><span class="line">    <span class="comment"># length is less than the specified length.</span></span><br><span class="line">    <span class="comment"># Account for [CLS], [SEP], [SEP] with "- 3" </span></span><br><span class="line">    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - <span class="number">3</span>) <span class="comment">#truncate the squence if it is too long</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Account for [CLS] and [SEP] with "- 2"</span></span><br><span class="line">    <span class="keyword">if</span> len(tokens_a) &gt; max_seq_length - <span class="number">2</span>:</span><br><span class="line">      tokens_a = tokens_a[<span class="number">0</span>:(max_seq_length - <span class="number">2</span>)]</span><br></pre></td></tr></table></figure>
<p>As commented above, this function Converts a single InputExample into a single InputFeatures. Don’t worry if this doesn’t make any sense for you, I will include a visual example later. But before, there are several variables you need to know:</p>
<ul>
<li>input_ids - Indices of input sequence tokens in the vocabulary (vocab.txt file in the BERT folder)</li>
<li>input_mask - Use to indicate which words will be put on “masks.” Words that are unmasked will not be used for self-attention training later</li>
<li>segment_ids - Use to indicate different sentences in the training process. 0 would indicate the word is in the first sentence, and 1 the second.<br>These three are essentially the most important information BERT needs for our data to be fed into its neural network. As you can see after this information is initiated, BERT tries to tokenize text_a and text_b (if the latter exists) using its own tokenizer function. It is not very different from normal tokenizers except it uses something called the wordpiece technique. It achieves the effect of tokenizing “Jacksonville” into “jack”, “##son”, “##ville,” thereby having smaller and more tokens to feed into the neural network. If you wish to see how to word piece is used, you can step into the tokenizer function to see how it does that (spoiler alert: they use greedy algorithm).</li>
</ul>
<p>Taken together, what this function does essentially is that it creates a label_map_  and tokenize the words using word piece. It considers whether the second sentence is present. And add the CLS and SEP token in places they should be.</p>
<ul>
<li>CLS stands for classification - it is used in classification tasks and placed as the first token in the list </li>
<li>SEP stands for separation - it is placed between the two sentences to indicate where the first sentence ends</li>
</ul>
<p>The following code block is pretty intuitive and easy to understand - it accomplishes these tasks</p>
<ul>
<li>Add CLS to the beginning </li>
<li>Add segment_id_, which is all 0 for the first sentence</li>
<li>Add SEP to indicate the end of the sentence</li>
<li>Do the same for the second sentence</li>
<li>For easy look up the vocal.txt and convert the word to their ids<br>This will pre-process the data into features that can be fed into BERT’s API</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">tokens = []</span><br><span class="line">  segment_ids = []</span><br><span class="line">  tokens.append(<span class="string">"[CLS]"</span>)</span><br><span class="line">  segment_ids.append(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">for</span> token <span class="keyword">in</span> tokens_a:</span><br><span class="line">    tokens.append(token)</span><br><span class="line">    segment_ids.append(<span class="number">0</span>)</span><br><span class="line">  tokens.append(<span class="string">"[SEP]"</span>)</span><br><span class="line">  segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> tokens_b:</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens_b:</span><br><span class="line">      tokens.append(token)</span><br><span class="line">      segment_ids.append(<span class="number">1</span>)</span><br><span class="line">    tokens.append(<span class="string">"[SEP]"</span>)</span><br><span class="line">    segment_ids.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  input_ids = tokenizer.convert_tokens_to_ids(tokens) <span class="comment">#jc: change them to indexes for easy look up</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># The mask has 1 for real tokens and 0 for padding tokens. Only real</span></span><br><span class="line">  <span class="comment"># tokens are attended to.</span></span><br><span class="line">  input_mask = [<span class="number">1</span>] * len(input_ids) <span class="comment">#We do not want to do self-attention for paddings, so we put 0 input mask</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Zero-pad up to the sequence length.</span></span><br><span class="line">  <span class="keyword">while</span> len(input_ids) &lt; max_seq_length: <span class="comment">#making sure all input has the same size(128 in my case)</span></span><br><span class="line">    input_ids.append(<span class="number">0</span>)</span><br><span class="line">    input_mask.append(<span class="number">0</span>)</span><br><span class="line">    segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">  <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">  <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line"></span><br><span class="line">  label_id = label_map[example.label]</span><br><span class="line">  <span class="keyword">if</span> ex_index &lt; <span class="number">5</span>: <span class="comment">#打印一些例子</span></span><br><span class="line">    tf.logging.info(<span class="string">"*** Example ***"</span>)</span><br><span class="line">    tf.logging.info(<span class="string">"guid: %s"</span> % (example.guid))</span><br><span class="line">    tf.logging.info(<span class="string">"tokens: %s"</span> % <span class="string">" "</span>.join(</span><br><span class="line">        [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens]))</span><br><span class="line">    tf.logging.info(<span class="string">"input_ids: %s"</span> % <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_ids]))</span><br><span class="line">    tf.logging.info(<span class="string">"input_mask: %s"</span> % <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> input_mask]))</span><br><span class="line">    tf.logging.info(<span class="string">"segment_ids: %s"</span> % <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> segment_ids]))</span><br><span class="line">    tf.logging.info(<span class="string">"label: %s (id = %d)"</span> % (example.label, label_id))</span><br><span class="line"></span><br><span class="line">  feature = InputFeatures(</span><br><span class="line">      input_ids=input_ids,</span><br><span class="line">      input_mask=input_mask,</span><br><span class="line">      segment_ids=segment_ids,</span><br><span class="line">      label_id=label_id,</span><br><span class="line">      is_real_example=<span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">return</span> feature</span><br></pre></td></tr></table></figure>
<p>Then we go back to convert to feature function and read all the data into features by calling the previous function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create_int_feature</span><span class="params">(values)</span>:</span></span><br><span class="line">    f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))</span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line">  features = collections.OrderedDict()</span><br><span class="line">  features[<span class="string">"input_ids"</span>] = create_int_feature(feature.input_ids) <span class="comment">#jc: change input_ids to ints</span></span><br><span class="line">  features[<span class="string">"input_mask"</span>] = create_int_feature(feature.input_mask)</span><br><span class="line">  features[<span class="string">"segment_ids"</span>] = create_int_feature(feature.segment_ids)</span><br><span class="line">  features[<span class="string">"label_ids"</span>] = create_int_feature([feature.label_id])</span><br><span class="line">  features[<span class="string">"is_real_example"</span>] = create_int_feature(</span><br><span class="line">      [int(feature.is_real_example)])</span><br><span class="line"></span><br><span class="line">  tf_example = tf.train.Example(features=tf.train.Features(feature=features))</span><br><span class="line">  writer.write(tf_example.SerializeToString())</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>Now we move to the only place in the run_classifier.py program you need to modify for your classification task, and the only thing you need to do is to finish the DataProcessor template, which pre-processes your data before feeding them into BERT’s neural network. So below is the template:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataProcessor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Base class for data converters for sequence classification data sets."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_train_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the train set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_dev_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the dev set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_test_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for prediction."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_labels</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets the list of labels for this data set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_read_tsv</span><span class="params">(cls, input_file, quotechar=None)</span>:</span></span><br><span class="line">    <span class="string">"""Reads a tab separated value file."""</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.Open(input_file, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">      reader = csv.reader(f, delimiter=<span class="string">"\t"</span>, quotechar=quotechar)</span><br><span class="line">      lines = []</span><br><span class="line">      <span class="keyword">for</span> line <span class="keyword">in</span> reader:</span><br><span class="line">        lines.append(line)</span><br><span class="line">      <span class="keyword">return</span> lines</span><br></pre></td></tr></table></figure>
<p>If you finish writing this class, you have a fine-tuning program ready to run for BERT! In the next part, I will show you a short demo that shows how to use run_classifier to compare similarity between two sentences. </p>
<h2 id="Demo-similarity-between-two-sentences"><a href="#Demo-similarity-between-two-sentences" class="headerlink" title="Demo: similarity between two sentences"></a>Demo: similarity between two sentences</h2><p>For this demo, I want to compare if two sentences are semantically the same despite having different wordings. The data I obtained came from Ant Group, a fintech company in China (it recently went public and has probably one of the biggest IPOs in China’s history). So each line in the data file consists of an index, two sentences, and either 1 or 0 indicating whether they mean the same. It looks like this:</p>
<p><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f50eff6160a154a6740226a.png" alt=""><br>The data are randomly put into three sets - the training data, testing data, and evaluation data. </p>
<p>The first and only step I took was to rewrite the DataProcessor class, which you can see below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># My own processor designed based on the template</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfProcessor</span><span class="params">(DataProcessor)</span>:</span></span><br><span class="line">    <span class="string">"""Processor for the CoLA data set (GLUE version)."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_train_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">        file_path = os.path.join(data_dir, <span class="string">'/Users/jiahuichen/PycharmProjects/BERT开源项目及数据/GLUE/glue_data/SIM/train.csv'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = f.readlines()</span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> index, line <span class="keyword">in</span> enumerate(reader):</span><br><span class="line">            guid = <span class="string">'train-%d'</span> % index</span><br><span class="line">            split_line = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            print(split_line)</span><br><span class="line">            text_a = tokenization.convert_to_unicode(split_line[<span class="number">1</span>])</span><br><span class="line">            text_b = tokenization.convert_to_unicode(split_line[<span class="number">2</span>])</span><br><span class="line">            label = split_line[<span class="number">3</span>]</span><br><span class="line">            examples.append(InputExample(guid=guid, text_a=text_a,</span><br><span class="line">                                         text_b=text_b, label=label))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_dev_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">        file_path = os.path.join(data_dir, <span class="string">'/Users/jiahuichen/PycharmProjects/BERT开源项目及数据/GLUE/glue_data/SIM/val.csv'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = f.readlines()</span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> index, line <span class="keyword">in</span> enumerate(reader):</span><br><span class="line">            guid = <span class="string">'train-%d'</span> % index</span><br><span class="line">            split_line = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            text_a = tokenization.convert_to_unicode(split_line[<span class="number">1</span>])</span><br><span class="line">            text_b = tokenization.convert_to_unicode(split_line[<span class="number">2</span>])</span><br><span class="line">            label = split_line[<span class="number">3</span>]</span><br><span class="line">            examples.append(InputExample(guid=guid, text_a=text_a,</span><br><span class="line">                                         text_b=text_b, label=label))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_test_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">        <span class="string">"""See base class."""</span></span><br><span class="line">        file_path = os.path.join(data_dir, <span class="string">'/Users/jiahuichen/PycharmProjects/BERT开源项目及数据/GLUE/glue_data/SIM/test.csv'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = f.readlines()</span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> index, line <span class="keyword">in</span> enumerate(reader):</span><br><span class="line">            guid = <span class="string">'train-%d'</span> % index</span><br><span class="line">            split_line = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            text_a = tokenization.convert_to_unicode(split_line[<span class="number">1</span>])</span><br><span class="line">            text_b = tokenization.convert_to_unicode(split_line[<span class="number">2</span>])</span><br><span class="line">            label = split_line[<span class="number">3</span>]</span><br><span class="line">            examples.append(InputExample(guid=guid, text_a=text_a,</span><br><span class="line">                                         text_b=text_b, label=label))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_labels</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""See base class."""</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">"0"</span>, <span class="string">"1"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_examples</span><span class="params">(self, lines, set_type)</span>:</span></span><br><span class="line">        <span class="string">"""Creates examples for the training and dev sets."""</span></span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> (i, line) <span class="keyword">in</span> enumerate(lines):</span><br><span class="line">            <span class="comment"># Only the test set has a header</span></span><br><span class="line">            <span class="keyword">if</span> set_type == <span class="string">"test"</span> <span class="keyword">and</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            guid = <span class="string">"%s-%s"</span> % (set_type, i)</span><br><span class="line">            <span class="keyword">if</span> set_type == <span class="string">"test"</span>:</span><br><span class="line">                text_a = tokenization.convert_to_unicode(line[<span class="number">2</span>])</span><br><span class="line">                label = <span class="string">"0"</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                text_a = tokenization.convert_to_unicode(line[<span class="number">2</span>])</span><br><span class="line">                label = tokenization.convert_to_unicode(line[<span class="number">4</span>])</span><br><span class="line">            examples.append(</span><br><span class="line">                InputExample(guid=guid, text_a=text_a, text_b=<span class="literal">None</span>, label=label))</span><br><span class="line">        <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<p>The three functions for getting the training, testing, and evaluation data set are essentially the same; so I will go over only the get_train_examples function. It firsts define a file path (where the data is on your computer), and opens the file with that path. It then uses the split function to split the line into three elements we want: the first sentence, and the second sentence, and the label. Finally, it stores them into an InputExample type, which is then ready to be fed into BERT. The result of the pre-processing looks like this</p>
<p><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f50eff6160a154a67402265.png" alt=""><br>The last thing to do before running the programs is adjusting the parameters. You can do this in most Python IDEs. I use PyCharm, which has the “Edit Run” option that lets you save the custom run parameters. The following are the parameters I set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--task_name=sim <span class="comment">#which task you want it to perform</span></span><br><span class="line">--do_train=true <span class="comment">#whether you want to do the training</span></span><br><span class="line">--do_eval=true <span class="comment">#whether you want to do the evaluation</span></span><br><span class="line">--data_dir=../GLUE/glue_data/SIM <span class="comment">#the directory for your data</span></span><br><span class="line">--vocab_file=../GLUE/BERT_BASE_DIR/chinese_L<span class="number">-12</span>_H<span class="number">-768</span>_A<span class="number">-12</span>/vocab.txt <span class="comment">#the directory for BERT's vocab file</span></span><br><span class="line">--bert_config_file=../GLUE/BERT_BASE_DIR/chinese_L<span class="number">-12</span>_H<span class="number">-768</span>_A<span class="number">-12</span>/bert_config.json <span class="comment"># the directory for the config file</span></span><br><span class="line">--init_checkpoint=../GLUE/BERT_BASE_DIR/chinese_L<span class="number">-12</span>_H<span class="number">-768</span>_A<span class="number">-12</span>/bert_model.ckpt <span class="comment"># the directory for the checkpoint file for Tensorflow</span></span><br><span class="line">--max_seq_length=<span class="number">128</span> <span class="comment"># the max sequence for a line. I will truncate if the line exceeds this limit and add paddings if below </span></span><br><span class="line">--train_batch_size=<span class="number">32</span> <span class="comment"># the batch size. I would recommend using a smaller batch size to prevent crashing</span></span><br><span class="line">--learning_rate=<span class="number">2e-5</span> <span class="comment"># the learning rate for gradient descent </span></span><br><span class="line">--num_train_epochs=<span class="number">2.0</span> <span class="comment">#numbers of epochs</span></span><br><span class="line">--output_dir=../GLUE/sim_output <span class="comment">#the directory where you want to store the output model</span></span><br></pre></td></tr></table></figure>
<p>With that, you are ready to hit the run button! It will take a while for the program to run depending on how large your data is and the computing power of your computer. On my MacBook Pro, it ran pretty fast, and after running it gives me the following evaluation results:</p>
<p><img src= "/img/loading.gif" data-src="https://pic.imgdb.cn/item/5f50eff6160a154a67402268.png" alt=""><br>As you can see, the accuracy is 0.75 at the end, which definitely has room for improvement. At the same time, it is important to note that my training data set only has 144 entries - it would be impossible for any previous NLP model to achieve the same performance with so few data. This speaks to the power of BERT. With a comprehensively pre-trained model, it needs far fewer task-specific data in the fine-tuning stage to achieve state-of-art performance. </p>
<p>I hope this walkthrough and demo is helpful for you:) If you have any questions or suggestions, you can reach me at <a href="mailto:jchen23@stanford.edu">jchen23@stanford.edu</a>.</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jiahui Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/08/25/Fine-tuning-BERT-Classifier/">http://yoursite.com/2020/08/25/Fine-tuning-BERT-Classifier/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://pic.imgdb.cn/item/5f38ac4514195aa594385bf6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2020/08/16/A-Tale-of-Three-Cities-%E4%B8%89%E5%9F%8E%E8%AE%B0/"><img class="next-cover" data-src="https://pic.imgdb.cn/item/5f38ac4514195aa594385bf6.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">A Tale of Three Cities | 三城记</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Jiahui Chen</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script></body></html>